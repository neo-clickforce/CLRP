{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-03T01:57:43.406322Z","iopub.execute_input":"2021-08-03T01:57:43.406731Z","iopub.status.idle":"2021-08-03T01:57:43.416129Z","shell.execute_reply.started":"2021-08-03T01:57:43.406695Z","shell.execute_reply":"2021-08-03T01:57:43.414926Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n/kaggle/input/commonlitreadabilityprize/train.csv\n/kaggle/input/commonlitreadabilityprize/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CommonLit Readability Prize\n\nCompetition URL: https://www.kaggle.com/c/commonlitreadabilityprize/overview\n\n**code credits**\\\nmodel training: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune \\\npooling method: https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently \\\nfeature build: https://www.kaggle.com/maximkazantsev/commonlit-readability-univ-sent-encoder-xgb \\\nmodel saving for future use: https://www.kaggle.com/questions-and-answers/92749 \\\nother: https://www.kaggle.com/vineethakkinapalli/clrp-roberta-base-representation-techniques\n\nIn this notebook I try to combine features extracted from excerpt into the model training process. There are 3 basic model I used, RoBertA with last 4 layers representations, RoBertA with a attention head, and Attention Pooling method on RoBertA.\nThe submission answer is the average value of all 15 models (each model has five submodels).","metadata":{}},{"cell_type":"code","source":"!pip3 install accelerate","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:57:46.383337Z","iopub.execute_input":"2021-08-03T01:57:46.383676Z","iopub.status.idle":"2021-08-03T01:57:52.905533Z","shell.execute_reply.started":"2021-08-03T01:57:46.383645Z","shell.execute_reply":"2021-08-03T01:57:52.904453Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (0.3.0)\nRequirement already satisfied: pyaml>=20.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (20.4.0)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.7.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from pyaml>=20.4.0->accelerate) (5.4.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (1.19.5)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport numpy as np\nimport re\nimport seaborn as sns\nfrom transformers import AutoModel,AutoConfig,AutoTokenizer,get_cosine_schedule_with_warmup\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom accelerate import Accelerator\nimport random\nimport os\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport joblib\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:57:52.909311Z","iopub.execute_input":"2021-08-03T01:57:52.909619Z","iopub.status.idle":"2021-08-03T01:57:52.920649Z","shell.execute_reply.started":"2021-08-03T01:57:52.909589Z","shell.execute_reply":"2021-08-03T01:57:52.919681Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsample = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:57:54.156456Z","iopub.execute_input":"2021-08-03T01:57:54.159410Z","iopub.status.idle":"2021-08-03T01:57:54.245167Z","shell.execute_reply.started":"2021-08-03T01:57:54.159300Z","shell.execute_reply":"2021-08-03T01:57:54.244191Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#train data excerpt length\ntrain[\"len\"] = train.excerpt.apply(len)\ntest[\"len\"] = test.excerpt.apply(len)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:57:54.508670Z","iopub.execute_input":"2021-08-03T01:57:54.511170Z","iopub.status.idle":"2021-08-03T01:57:54.523014Z","shell.execute_reply.started":"2021-08-03T01:57:54.511123Z","shell.execute_reply":"2021-08-03T01:57:54.522018Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train[\"excerpt\"] = train.excerpt.apply(lambda x: x.replace(\"\\n\",\" \"))\ntest[\"excerpt\"] = test.excerpt.apply(lambda x: x.replace(\"\\n\",\" \"))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:57:54.963369Z","iopub.execute_input":"2021-08-03T01:57:54.963727Z","iopub.status.idle":"2021-08-03T01:57:54.972198Z","shell.execute_reply.started":"2021-08-03T01:57:54.963698Z","shell.execute_reply":"2021-08-03T01:57:54.971311Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def feature(excerpt, stop_words):\n    dialogue_counts=excerpt.count('\"')/2\n    num_sentence = len(sent_tokenize(excerpt))\n    \n    words = word_tokenize(re.sub(\"[^a-zA-Z]\", \" \", excerpt).lower())\n    initial_num_words = len(words)\n    target_words = [word for word in words if word not in stop_words]\n    processed_num_words=len(target_words)\n    text_shrinkage = processed_num_words/initial_num_words\n    avg_sentence_length = initial_num_words / num_sentence\n    \n    return dialogue_counts, num_sentence, processed_num_words, text_shrinkage, avg_sentence_length","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:57:55.872858Z","iopub.execute_input":"2021-08-03T01:57:55.873218Z","iopub.status.idle":"2021-08-03T01:57:55.879566Z","shell.execute_reply.started":"2021-08-03T01:57:55.873184Z","shell.execute_reply":"2021-08-03T01:57:55.878553Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\"))\ntrain['feature'] = train.excerpt.apply(feature, stop_words= stop_words)\ntest['feature'] = test.excerpt.apply(feature, stop_words= stop_words)\ntrain[\"dialogue\"], train[\"num_sentence\"], train[\"num_processed_words\"], train['text_shrinkage'], \\\ntrain[\"avg_sentence_length\"] = zip(*train.feature)\n\ntest[\"dialogue\"], test[\"num_sentence\"], test[\"num_processed_words\"], test['text_shrinkage'], \\\ntest[\"avg_sentence_length\"] = zip(*test.feature)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:57:56.393219Z","iopub.execute_input":"2021-08-03T01:57:56.393568Z","iopub.status.idle":"2021-08-03T01:58:01.936628Z","shell.execute_reply.started":"2021-08-03T01:57:56.393536Z","shell.execute_reply":"2021-08-03T01:58:01.935695Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=\"feature\", inplace=True)\ntest.drop(columns=\"feature\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:01.938043Z","iopub.execute_input":"2021-08-03T01:58:01.938428Z","iopub.status.idle":"2021-08-03T01:58:01.948201Z","shell.execute_reply.started":"2021-08-03T01:58:01.938359Z","shell.execute_reply":"2021-08-03T01:58:01.947445Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,feature,max_len=256, train=True):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.train = train\n        if self.train:\n            self.targets = df['target'].to_numpy()\n        self.feature = feature\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        feature = torch.tensor(self.feature[idx],dtype=torch.float,)\n        \n        if self.train:\n            target = torch.tensor(self.targets[idx],dtype=torch.float) \n            return encode, feature, target\n        else:\n            return encode, feature\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:01.949992Z","iopub.execute_input":"2021-08-03T01:58:01.950408Z","iopub.status.idle":"2021-08-03T01:58:01.959158Z","shell.execute_reply.started":"2021-08-03T01:58:01.950339Z","shell.execute_reply":"2021-08-03T01:58:01.958142Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"config = {\n    'lr': 2e-5,\n    'wd':0.01,\n    'batch_size':16,\n    'valid_step':10,\n    'max_len':500,\n    'epochs':5,\n    'nfolds':5,\n    'seed':9527,\n    'model_path':'roberta-base',\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=config['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:01.960525Z","iopub.execute_input":"2021-08-03T01:58:01.960892Z","iopub.status.idle":"2021-08-03T01:58:01.973540Z","shell.execute_reply.started":"2021-08-03T01:58:01.960855Z","shell.execute_reply":"2021-08-03T01:58:01.972632Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"for i in range(config['nfolds']):\n    os.makedirs(f'concat_model{i}',exist_ok=True)\nfor i in range(config['nfolds']):\n    os.makedirs(f'atHead_model{i}',exist_ok=True)\nfor i in range(config['nfolds']):\n    os.makedirs(f'atPool_model{i}',exist_ok=True)\nos.makedirs('scaler',exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:01.974774Z","iopub.execute_input":"2021-08-03T01:58:01.975242Z","iopub.status.idle":"2021-08-03T01:58:01.982298Z","shell.execute_reply.started":"2021-08-03T01:58:01.975206Z","shell.execute_reply":"2021-08-03T01:58:01.981197Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"num_bins = int(np.floor(1 + np.log2(len(train))))\ntrain.loc[:,'bins'] = pd.cut(train['target'],bins=num_bins,labels=False)\nbins = train.bins.to_numpy()\n\ntrain['Fold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train, y=bins)):\n    train.loc[valid_idx,'Fold'] = k","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:04.893003Z","iopub.execute_input":"2021-08-03T01:58:04.893334Z","iopub.status.idle":"2021-08-03T01:58:04.911496Z","shell.execute_reply.started":"2021-08-03T01:58:04.893301Z","shell.execute_reply":"2021-08-03T01:58:04.910243Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,path):\n        super(Model,self).__init__()\n        self.config = AutoConfig.from_pretrained(path)\n        self.config.update({'output_hidden_states':True,\"hidden_dropout_prob\": 0.0})\n        self.roberta = AutoModel.from_pretrained(path,config=self.config)\n        self.linear = nn.Linear(self.config.hidden_size*4+5, 1, 1)\n\n    def forward(self,feature, **xb):\n        x = self.roberta(**xb)\n        x = torch.stack(x[2])\n        x = torch.cat((x[-1], x[-2], x[-3], x[-4]),-1)\n        x = x[:, 0]\n        x = torch.cat((x,feature), -1)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:07.173169Z","iopub.execute_input":"2021-08-03T01:58:07.173525Z","iopub.status.idle":"2021-08-03T01:58:07.180786Z","shell.execute_reply.started":"2021-08-03T01:58:07.173493Z","shell.execute_reply":"2021-08-03T01:58:07.179686Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super(AttentionHead, self).__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \nclass ATHeadModel(nn.Module):\n    def __init__(self,path):\n        super(ATHeadModel,self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)  \n        self.config = AutoConfig.from_pretrained(path)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size+5,1)\n\n    def forward(self,feature,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = torch.cat((x,feature), -1)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:07.560823Z","iopub.execute_input":"2021-08-03T01:58:07.561113Z","iopub.status.idle":"2021-08-03T01:58:07.570887Z","shell.execute_reply.started":"2021-08-03T01:58:07.561085Z","shell.execute_reply":"2021-08-03T01:58:07.569848Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class AttentionPoolingModel(nn.Module):\n    def __init__(self, path):\n        super(AttentionPoolingModel, self).__init__() \n        self.config = AutoConfig.from_pretrained(path)\n        self.config.update({'output_hidden_states':True,\"hidden_dropout_prob\": 0.0})\n        self.roberta = AutoModel.from_pretrained(path,config=self.config)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size+5,1)\n\n        q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.config.hidden_size))\n        self.q = nn.Parameter(torch.from_numpy(q_t).float())\n        w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.config.hidden_size, self.config.hidden_size))\n        self.w_h = nn.Parameter(torch.from_numpy(w_ht).float())\n\n    def forward(self, feature, **xb):\n        x = self.roberta(**xb)\n        x = torch.stack(x[2])\n        x = torch.stack([x[layer_i][:, 0].squeeze() for layer_i in range(1, self.config.num_hidden_layers+1)], dim=-1)\n        x = x.view(-1, self.config.num_hidden_layers, self.config.hidden_size)\n        x = self.attention(x)\n        x = self.dropout(x)\n        x = torch.cat((x,feature), -1)\n        x = self.linear(x)\n        return x\n\n    def attention(self, h):\n        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n        v = F.softmax(v, -1)\n        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n        return v","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:09.371939Z","iopub.execute_input":"2021-08-03T01:58:09.372275Z","iopub.status.idle":"2021-08-03T01:58:09.384655Z","shell.execute_reply.started":"2021-08-03T01:58:09.372237Z","shell.execute_reply":"2021-08-03T01:58:09.383594Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def run(fold, MODEL, verbose=True):\n    \n    \n    model_dict = {\"concat_model\":Model,\n                  \"atHead_model\":ATHeadModel,\n                  \"atPool_model\":AttentionPoolingModel}\n    \n    def loss_fn(outputs,targets):\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))\n    \n    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n        train_loss = 0\n        for i, (inputs1,feature1,targets1) in enumerate(train_loader):\n            model.train()\n            optimizer.zero_grad()\n            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n            outputs1 = model(feature1, **inputs1)\n            loss1 = loss_fn(outputs1,targets1)\n            loss1.backward()\n            optimizer.step()\n            \n            train_loss += loss1.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n            \n            #evaluating for every valid_step\n            #if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n            if (i + 1) == len(train_loader):\n                model.eval()\n                valid_loss = 0\n                with torch.no_grad():\n                    for j, (inputs2,feature2,targets2) in enumerate(valid_loader):\n                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n                        outputs2 = model(feature2,**inputs2)\n                        loss2 = loss_fn(outputs2,targets2)\n                        valid_loss += loss2.item()\n                     \n                    valid_loss /= len(valid_loader)\n                    if valid_loss <= best_loss:\n                        if verbose:\n                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n                        best_loss = valid_loss\n                        torch.save(model.state_dict(),f'./{MODEL}{fold}/model.bin')\n                        tokenizer.save_pretrained(f'./{MODEL}{fold}')\n                        \n        return best_loss\n    \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n    \n    x_train,x_valid = train.query(f\"Fold != {fold}\"),train.query(f\"Fold == {fold}\")\n    \n    if os.path.isfile(f\"scaler/scaler_fold{fold}.gz\"):\n        scaler = joblib.load(f\"scaler/scaler_fold{fold}.gz\") \n    else:\n        scaler = StandardScaler()\n        scaler.fit(x_train.iloc[:,9:].to_numpy())\n        joblib.dump(scaler, f\"scaler/scaler_fold{fold}.gz\")\n    x_train_feature=scaler.transform(x_train.iloc[:,9:].to_numpy())\n    x_valid_feature=scaler.transform(x_valid.iloc[:,9:].to_numpy())\n    \n    \n    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n    model = model_dict[MODEL](config['model_path'])\n\n    train_ds = CLRPDataset(x_train,tokenizer,x_train_feature,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=True,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    valid_ds = CLRPDataset(x_valid,tokenizer,x_valid_feature,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    print(f\"Fold: {fold}\")\n    best_loss = 9999\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n                                            optimizer,epoch,fold,best_loss,\n                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:58:45.314143Z","iopub.execute_input":"2021-08-03T01:58:45.314508Z","iopub.status.idle":"2021-08-03T01:58:45.333755Z","shell.execute_reply.started":"2021-08-03T01:58:45.314474Z","shell.execute_reply":"2021-08-03T01:58:45.332938Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def get_prediction(df,Fold,MODEL,path,model_path,device='cuda'):\n    outputs = np.zeros(len(df))\n    for f in range(Fold):\n        accelerator = Accelerator()\n        model = MODEL(model_path)\n        model.load_state_dict(torch.load(path+f\"{f}/model.bin\",map_location=device))\n        model.eval()\n        \n\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        \n        scaler = joblib.load(f\"scaler/scaler_fold{f}.gz\")\n        df_feature = scaler.transform(df.iloc[:,5:].to_numpy())\n\n        test_ds = CLRPDataset(df,tokenizer,df_feature, config['max_len'],train=False)\n        test_dl = DataLoader(test_ds,\n                            batch_size = config[\"batch_size\"],\n                            shuffle=False,\n                            num_workers = 4,\n                            pin_memory=True)\n        \n        model,test_dl = accelerator.prepare(model,test_dl)\n\n        predictions = list()\n        for i, (inputs, feature) in enumerate(test_dl):\n            inputs = {key:val.reshape(val.shape[0],-1) for key,val in inputs.items()}\n            outputs = model(feature, **inputs)\n            outputs = outputs.cpu().detach().numpy().ravel().tolist()\n            predictions.extend(outputs)\n\n        torch.cuda.empty_cache()\n        outputs+=np.array(predictions)\n    return outputs/Fold","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:59:05.561448Z","iopub.execute_input":"2021-08-03T01:59:05.561778Z","iopub.status.idle":"2021-08-03T01:59:05.573138Z","shell.execute_reply.started":"2021-08-03T01:59:05.561747Z","shell.execute_reply":"2021-08-03T01:59:05.571916Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"for f in range(config['nfolds']):\n    run(f, 'concat_model')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T01:59:15.591030Z","iopub.execute_input":"2021-08-03T01:59:15.591353Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda is used\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2807a45068c14ee38f09e8c4c1d37fbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9f99352eb542db82694eeaeba7f9c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943d09fcb0894838a8669c720f812453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b082acfdfb47498481c844256a3fdc80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008f374d83e14e38b61b6278fdb8abf5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Fold: 0\nEpoch Started:0\nepoch:0 | Train Loss:0.6861157102484099 | Validation loss:0.5458745484550794\n\u001b[32mValidation loss Decreased from 9999 to 0.5458745484550794\u001b[0m\nEpoch Started:1\nepoch:1 | Train Loss:0.4556661145368093 | Validation loss:0.5264163936177889\n\u001b[32mValidation loss Decreased from 0.5458745484550794 to 0.5264163936177889\u001b[0m\nEpoch Started:2\nepoch:2 | Train Loss:0.33715694544600766 | Validation loss:0.4989985591835446\n\u001b[32mValidation loss Decreased from 0.5264163936177889 to 0.4989985591835446\u001b[0m\nEpoch Started:3\nepoch:3 | Train Loss:0.2361377477645874 | Validation loss:0.4910426595144802\n\u001b[32mValidation loss Decreased from 0.4989985591835446 to 0.4910426595144802\u001b[0m\nEpoch Started:4\ncuda is used\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Fold: 1\nEpoch Started:0\nepoch:0 | Train Loss:0.6750002757344448 | Validation loss:0.49220481763283414\n\u001b[32mValidation loss Decreased from 9999 to 0.49220481763283414\u001b[0m\nEpoch Started:1\nepoch:1 | Train Loss:0.4402375724953665 | Validation loss:0.4789656044708358\n\u001b[32mValidation loss Decreased from 0.49220481763283414 to 0.4789656044708358\u001b[0m\nEpoch Started:2\nEpoch Started:3\nEpoch Started:4\ncuda is used\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Fold: 2\nEpoch Started:0\nepoch:0 | Train Loss:0.7102370883377505 | Validation loss:0.5416221668322881\n\u001b[32mValidation loss Decreased from 9999 to 0.5416221668322881\u001b[0m\nEpoch Started:1\nepoch:1 | Train Loss:0.45342342471572716 | Validation loss:0.5234515708353784\n\u001b[32mValidation loss Decreased from 0.5416221668322881 to 0.5234515708353784\u001b[0m\nEpoch Started:2\nEpoch Started:3\nepoch:3 | Train Loss:0.24119354165355925 | Validation loss:0.4954050588938925\n\u001b[32mValidation loss Decreased from 0.5234515708353784 to 0.4954050588938925\u001b[0m\nEpoch Started:4\ncuda is used\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Fold: 3\nEpoch Started:0\n","output_type":"stream"}]},{"cell_type":"code","source":"for f in range(config['nfolds']):\n    run(f, 'atHead_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in range(config['nfolds']):\n    run(f, 'atPool_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer1=get_prediction(test,config[\"nfolds\"],Model,\"concat_model\",config['model_path'],device='cuda')\nanswer2=get_prediction(test,config[\"nfolds\"],ATHeadModel,\"atHead_model\",config['model_path'],device='cuda')\nanswer3=get_prediction(test,config[\"nfolds\"],AttentionPoolingModel,\"atPool_model\",config['model_path'],device='cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer=(answer1+answer2+answer3)/3\nsample['target']=answer\nsample.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}